# -*- coding: utf-8 -*-
"""LFIR Project_Image classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KzuzsdPA74nGWDSAYGwq59EEzQZRVew8

Make sure you are connected to a T4 GPU runtime. The following code should report true if you are.
"""

from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
import torch
from PIL import Image
import os
import re

# Load model, tokenizer, and image processor
model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
image_processor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

# unzip ASL_Dataset.zip
# rm ASL_Dataset.zip

from torch.utils.data import DataLoader, Dataset

class ImageCaptionDataset(Dataset):
    def __init__(self, image_dir):
        self.image_dir = image_dir
        self.image_paths = [os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith(('.png', '.jpg', '.jpeg'))]
        self.captions = [re.sub(r'\(.*?\)', '', os.path.splitext(os.path.basename(path))[0]).strip() for path in self.image_paths]

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        caption = self.captions[idx]
        image = Image.open(image_path)
        if image.mode != "RGB":
            image = image.convert(mode="RGB")
        pixel_values = image_processor(images=image, return_tensors="pt", size=(224, 224)).pixel_values.squeeze()
        input_ids = tokenizer(caption, return_tensors="pt", padding="max_length", truncation=True, max_length=16).input_ids.squeeze()
        return pixel_values, input_ids

# Create dataset and dataloader
dataset = ImageCaptionDataset("/home/nnm5qe/ASL Dataset")
dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

# Training parameters
num_epochs = 100
learning_rate = 5e-5

# Optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    model.train()  # Set the model to training mode
    total_loss = 0

    for batch in dataloader:
        pixel_values, input_ids = batch

        # Forward pass
        outputs = model(pixel_values=pixel_values, labels=input_ids)
        loss = outputs.loss

        # Backward pass
        loss.backward()

        # Update parameters
        optimizer.step()

        # Zero the gradients
        optimizer.zero_grad()

        # Accumulate loss
        total_loss += loss.item()

    # Print average loss for the epoch
    avg_loss = total_loss / len(dataloader)
    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}")

# Save the model
model.save_pretrained("ASL_Translation_Model")
tokenizer.save_pretrained("ASL_Translation_Tokenizer")

# # Load the saved model and tokenizer
# model = VisionEncoderDecoderModel.from_pretrained("/content/path_to_save_model")
# tokenizer = AutoTokenizer.from_pretrained("/content/path_to_save_tokenizer")
# image_processor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

# # Preprocess the image
# def preprocess_image(image_path):
#     image = Image.open(image_path)
#     if image.mode != "RGB":
#         image = image.convert(mode="RGB")
#     pixel_values = image_processor(images=image, return_tensors="pt").pixel_values
#     return pixel_values

# image_path = input("Please enter the path to the image of the ASL command you wish to give.\n")
# pixel_values = preprocess_image(image_path)

# # Generate the caption
# model.eval()  # Set the model to evaluation mode
# with torch.no_grad():
#     generated_ids = model.generate(pixel_values)

# # Decode the generated ids to get the caption
# generated_caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
# print("Generated Caption:", generated_caption)